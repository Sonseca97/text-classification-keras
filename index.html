<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Documentation Text Classification Keras - Text Classification Library for Keras">
  
  <link rel="shortcut icon" href="./img/favicon.ico">
  <title>Home - Documentation Text Classification Keras</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="./css/theme.css" type="text/css" />
  <link rel="stylesheet" href="./css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="./css/highlight.css">
  <link href="./css/extras.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Home";
    var mkdocs_page_input_path = "index.md";
    var mkdocs_page_url = "/";
  </script>
  
  <script src="./js/jquery-2.1.1.min.js"></script>
  <script src="./js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="./js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="." class="icon icon-home"> Documentation Text Classification Keras</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1 current">
		
    <a class="current" href=".">Home</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#text-classification-keras">Text Classification Keras</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#quick-start">Quick start</a></li>
        
            <li><a class="toctree-l3" href="#api-documenation">API Documenation</a></li>
        
            <li><a class="toctree-l3" href="#advanced">Advanced</a></li>
        
            <li><a class="toctree-l3" href="#contributing">Contributing</a></li>
        
            <li><a class="toctree-l3" href="#acknowledgements">Acknowledgements</a></li>
        
            <li><a class="toctree-l3" href="#citation">Citation</a></li>
        
            <li><a class="toctree-l3" href="#license">License</a></li>
        
        </ul>
    

    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">API Docs</span>
    <ul class="subnav">
                <li class="">
                    
    <span class="caption-text">Models</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="texcla.models.sequence_encoders/">Sequence Processing Models</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="texcla.models.token_model/">Sequence Model Builder Factory</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="texcla.models.sentence_model/">Sentence Model Builder Factory</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="texcla.models.layers/">Custom Layers</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <span class="caption-text">Preprocessing</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="texcla.preprocessing.word_tokenizer/">Word Tokenizer</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="texcla.preprocessing.sentence_tokenizer/">Sentence Tokenizer</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="texcla.preprocessing.tokenizer/">Tokenizer</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="texcla.preprocessing.char_tokenizer/">Char Tokenizer</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="texcla.preprocessing.utils/">Utils</a>
                </li>
    </ul>
                </li>
                <li class="">
                    
    <a class="" href="texcla.embeddings/">Embeddings</a>
                </li>
                <li class="">
                    
    <a class="" href="texcla.experiment/">Experiment</a>
                </li>
                <li class="">
                    
    <a class="" href="texcla.corpus/">Corpus</a>
                </li>
                <li class="">
                    
    <a class="" href="texcla.data/">Data</a>
                </li>
                <li class="">
                    
    <span class="caption-text">Utils</span>
    <ul class="subnav">
                <li class="toctree-l3">
                    
    <a class="" href="texcla.utils.format/">Format</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="texcla.utils.generators/">Generators</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="texcla.utils.io/">IO</a>
                </li>
                <li class="toctree-l3">
                    
    <a class="" href="texcla.utils.sampling/">Sampling</a>
                </li>
    </ul>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href=".">Documentation Text Classification Keras</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".">Docs</a> &raquo;</li>
    
      
    
    <li>Home</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="http://github.com/jfilter/text-classification-keras/blob/master/docs/templates/index.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="text-classification-keras">Text Classification Keras <a href="https://travis-ci.com/jfilter/text-classification-keras"><img alt="Build Status" src="https://travis-ci.com/jfilter/text-classification-keras.svg?branch=master" /></a> <a href="https://pypi.org/project/text-classification-keras/"><img alt="PyPI" src="https://img.shields.io/pypi/v/text-classification-keras.svg" /></a> <a href="https://pypi.org/project/text-classification-keras/"><img alt="PyPI - Python Version" src="https://img.shields.io/pypi/pyversions/text-classification-keras.svg" /></a> <a href="https://gitter.im/text-classification-keras/Lobby"><img alt="Gitter" src="https://img.shields.io/gitter/room/text-classification-keras/Lobby.svg" /></a></h1>
<p>A high-level text classification library implementing various well-established models. With a clean and extendable interface to implement custom architectures.</p>
<h2 id="quick-start">Quick start</h2>
<h3 id="install">Install</h3>
<pre><code class="bash">pip install text-classification-keras[full]==0.1.0
</code></pre>

<p>The <code>[full]</code> will additionally install <a href="https://github.com/tensorflow/tensorflow">TensorFlow</a>, <a href="https://github.com/explosion/spaCy">Spacy</a>, and <a href="https://github.com/jfilter/text-classification-keras">Deep Plots</a>.</p>
<h3 id="usage">Usage</h3>
<pre><code class="python">from texcla import experiment, data
from texcla.models import TokenModelFactory, YoonKimCNN
from texcla.preprocessing import FastTextWikiTokenizer

# input text
X = ['some random text', 'another random text', 'peter', ...]

# input labels
y = ['a', 'b', 'a', ...]

# use the special tokenizer used for constructing the embeddings
tokenizer = FastTextWikiTokenizer()

# preprocess data (once)
experiment.setup_data(X, y, tokenizer, 'data.bin', max_len=100)

# load data
ds = data.Dataset.load('data.bin')

# construct base
factory = TokenModelFactory(
    ds.num_classes, ds.tokenizer.token_index, max_tokens=100,
    embedding_type='fasttext.wiki.simple', embedding_dims=300)

# choose a model
word_encoder_model = YoonKimCNN()

# build a model
model = factory.build_model(
    token_encoder_model=word_encoder_model, trainable_embeddings=False)

# use experiment.train as wrapper for Keras.fit()
experiment.train(x=ds.X, y=ds.y, validation_split=0.1, model=model,
    word_encoder_model=word_encoder_model)
</code></pre>

<p>Check out more <a href="./examples">examples</a>.</p>
<h2 id="api-documenation">API Documenation</h2>
<p><a href="https://github.io/jfilter/text-classification-keras/">https://github.io/jfilter/text-classification-keras/</a></p>
<h2 id="advanced">Advanced</h2>
<h3 id="embeddings">Embeddings</h3>
<p>Choose a pre-trained word embedding by setting the embedding_type and the corresponding embedding dimensions. Set <code>embedding_type=None</code> to use a randomly initialized word embedding.</p>
<pre><code class="python">factory = TokenModelFactory(embedding_type='fasttext.wiki.simple', embedding_dims=300)
</code></pre>

<h4 id="fasttext">FastText</h4>
<p>Several pre-trained FastText embeddings are included. For now, we only have the word embeddings and not the n-gram features. All embedding have 300 dimensions.</p>
<ul>
<li><a href="https://fasttext.cc/docs/en/english-vectors.html">English Vectors</a>: e.g. <code>fasttext.wn.1M.300d</code>, <a href="https://github.com/jfilter/text-classification-keras/blob/master/texcla/embeddings.py#L19">check out all avaiable embeddings</a></li>
<li><a href="https://fasttext.cc/docs/en/crawl-vectors.html">Multilang Vectors</a>: in the format <code>fasttext.cc.LANG_CODE</code> e.g. <code>fasttext.cc.en</code></li>
<li><a href="https://fasttext.cc/docs/en/pretrained-vectors.html">Wikipedia Vectors</a>: in the format <code>fasttext.wiki.LANG_CODE</code> e.g. <code>fasttext.wiki.en</code></li>
</ul>
<h4 id="glove">GloVe</h4>
<p>Predecessor to FastText. The dimension varies.</p>
<ul>
<li><a href="https://nlp.stanford.edu/projects/glove/">GloVe</a>: e.g. <code>glove.6B.50d</code>, <a href="https://github.com/jfilter/text-classification-keras/blob/master/texcla/embeddings.py#L19">check out all avaiable embeddings</a></li>
</ul>
<h3 id="tokenzation">Tokenzation</h3>
<ul>
<li>To work on token (or word) level, use a TokenTokenizer such e.g. <code>TwokenizeTokenizer</code> or <code>SpacyTokenizer</code>.</li>
<li>To work on token and sentence level, use <code>SpacySentenceTokenizer</code>.</li>
<li>To create an custom Tokenizer, extend <code>Tokenizer</code> and implement the <code>token_generator</code> method.</li>
</ul>
<h4 id="spacy">Spacy</h4>
<p>You may use Spacy for the tokenization. See instructions on how to
<a href="https://spacy.io/docs/usage/models#download">download model</a> for your target language.</p>
<pre><code class="bash">python -m spacy download en
</code></pre>

<h3 id="models">Models</h3>
<h4 id="token-based-models-words">Token-based Models (Words)</h4>
<p>When working on token level, use <code>TokenModelFactory</code>.</p>
<pre><code class="python">from keras_text.models import TokenModelFactory, YoonKimCNN

factory = TokenModelFactory(tokenizer.num_classes, tokenizer.token_index,
    max_tokens=100, embedding_type='glove.6B.100d')
word_encoder_model = YoonKimCNN()
model = factory.build_model(token_encoder_model=word_encoder_model)
</code></pre>

<p>Currently supported models include:</p>
<ul>
<li><a href="https://arxiv.org/abs/1408.5882">Yoon Kim CNN</a></li>
<li><a href="https://arxiv.org/abs/1312.6026">Stacked RNNs</a></li>
<li><a href="https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf">Attention (with/without context) based RNN encoders</a></li>
</ul>
<p><code>TokenModelFactory.build_model</code> uses the provided word encoder which is then classified via a <a href="https://keras.io/layers/core/#dense">Dense</a> layer.</p>
<h4 id="sentence-basded-models">Sentence-basded Models</h4>
<p>When working on token level, use <code>TokenModelFactory</code>.</p>
<pre><code class="python"># Pad max sentences per doc to 500 and max words per sentence to 200.
# Can also use `max_sents=None` to allow variable sized max_sents per mini-batch.

factory = SentenceModelFactory(10, tokenizer.token_index, max_sents=500,
    max_tokens=200, embedding_type='glove.6B.100d')
word_encoder_model = AttentionRNN()
sentence_encoder_model = AttentionRNN()

# Allows you to compose arbitrary word encoders followed by sentence encoder.
model = factory.build_model(word_encoder_model, sentence_encoder_model)
</code></pre>

<ul>
<li><a href="http://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf">Hierarchical attention networks</a>
    (HANs) can be build by composing two attention based RNN models. This is useful when a document is very large.</li>
<li>For smaller document a reasonable way to encode sentences is to average words within it. This can be done by using
    <code>token_encoder_model=AveragingEncoder()</code></li>
<li>Mix and match encoders as you see fit for your problem.</li>
</ul>
<p><code>SentenceModelFactory.build_model</code> created a tiered model where words within a sentence is first encoded using
<code>word_encoder_model</code>. All such encodings per sentence is then encoded using <code>sentence_encoder_model</code>.</p>
<h2 id="contributing">Contributing</h2>
<p>If you have a <strong>question</strong>, found a <strong>bug</strong> or want to propose a new <strong>feature</strong>, have a look at the <a href="https://github.com/jfilter/text-classification-keras/issues">issues page</a>.</p>
<p><strong>Pull requests</strong> are especially welcomed when they fix bugs or improve the code quality.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>Built upon the work by Raghavendra Kotikalapudi: <a href="https://github.com/raghakot/keras-text">keras-text</a>.</p>
<h2 id="citation">Citation</h2>
<p>If you find Text Classification Keras useful for an academic publication, then please use the following BibTeX to cite it:</p>
<pre><code class="tex">@misc{raghakotfiltertexclakeras
    title={Text Classification Keras},
    author={Raghavendra Kotikalapudi, and Johannes Filter, and contributors},
    year={2018},
    publisher={GitHub},
    howpublished={\url{https://github.com/jfilter/text-classification-keras}},
}
</code></pre>

<h2 id="license">License</h2>
<p>MIT.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="texcla.models.sequence_encoders/" class="btn btn-neutral float-right" title="Sequence Processing Models">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="http://github.com/jfilter/text-classification-keras/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
      
        <span style="margin-left: 15px"><a href="texcla.models.sequence_encoders/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '.';</script>
    <script src="./js/theme.js"></script>
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script src="./search/require.js"></script>
      <script src="./search/search.js"></script>

</body>
</html>

<!--
MkDocs version : 0.17.5
Build Date UTC : 2018-08-01 14:25:49
-->
